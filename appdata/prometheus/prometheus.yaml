# my global config
global:
  scrape_interval: 30s #Default is every 1 minute.
  evaluation_interval: 30s # Evaluate rules every 30 seconds. The default is every 1 minute.

# Load and evaluate rules in this file every 'evaluation_interval' seconds.
rule_files:
  - 'alert.rules.yaml'

alerting:
  alertmanagers:
  - scheme: http
    static_configs:
    - targets:
      - "alertmanager:9093"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.

  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  - job_name: "node_exporter"
    scrape_interval: 30s
    static_configs:
      - targets: ["node_exporter:9100"]
    metric_relabel_configs:
      - source_labels: [__name__]
        # node_memory_MemTotal_bytes`: Total memory in bytes.
        # node_memory_MemFree_bytes`: Free memory in bytes.
        # node_memory_Cached_bytes`: Amount of memory used for file caching in bytes.
        # node_filesystem_avail_bytes`: Available disk space in bytes.
        # node_filesystem_size_bytes`: Total disk space in bytes.
        # node_md_state `: indicates if the array is recovering using a spare, resyncing after an event such as an unclean shutdown, active and healthy, or disabled and inactive.
        # node_md_blocks AND node_md_blocks_synced `:  node_md_blocks and node_md_blocks_synced can tell you how the array getting synced back up is progressin
        # node_md_disks `: is the primary metric of interest as you will want to know if there are any failed disks, and depending on your the setup you may also know that there must be a minimum number of spares and/or active devices. A RAID1 array with only one active disk is not exactly in prime health after all.
        regex: '(node_((memory_(MemTotal|MemFree|Cached)_bytes)|(filesystem_(avail|size)_bytes)|(md_(state|blocks|blocks_synced|disks_required))))'
        action: replace
        target_label: ls_monitoring #the metrics which are labeled with the ls_monitoring=1 will be scraped by the federated (cloud) prometheus to save bandwith!
        replacement: '1'

  # - job_name: "docker"
  #   scrape_interval: 30s
  #   static_configs:
  #     - targets: ["host.docker.internal:9323"]

  - job_name: "graphitexporter"
    scrape_interval: 30s
    static_configs:
      - targets: ["graphiteexporter:9108"]

  - job_name: "cadvisor"
    scrape_interval: 30s
    static_configs:
      - targets: ["cadvisor:8080"]

  # - job_name: 'assistant.home'
  #   scrape_interval: 60s
  #   metrics_path: /api/prometheus

  #   # Long-Lived Access Token
  #   bearer_token: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJkNDAzYjIyZjE5Mzg0YjE5YTU5YmY1MDk5NzM5YzgwNyIsImlhdCI6MTY4NDczMzYxMywiZXhwIjoyMDAwMDkzNjEzfQ.ARcJoArobK3DS8z3T-8dX0AcKKwQUhd3Fyk1WAWw8tY'

  #   scheme: http
  #   static_configs:
  #     - targets: ['assistant.home:8123']

  - job_name: 'mikrotikexporter'
    static_configs:
      - targets: ['mikrotikexporter:49090']

  - job_name: blackbox
    metrics_path: /probe
    scrape_interval: 60s
    params:
      module: [http_2xx]
    static_configs:
    - targets:
      - nas.home
      - dns.home
      - hydra.home
      - radarr.home66.xyz
      - assistant.home66.xyz
      - grafana.home66.xyz
      - prometheus.home66.xyz
      - headphones.home66.xyz
      - nextcloud.home66.xyz
      - overseerr.home66.xyz
      - plex.home66.xyz
      - portainer.home66.xyz
      - prowlarr.home66.xyz
      - qbittorrent.home66.xyz
      - sonarr.home66.xyz
      - transmission.home66.xyz
    relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: blackbox:9115

